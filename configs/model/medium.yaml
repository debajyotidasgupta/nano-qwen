# Qwen3-8B-like dense model
hidden_size: 4096
num_hidden_layers: 36
num_attention_heads: 32
num_key_value_heads: 8
intermediate_size: 12288
vocab_size: 151936
max_position_embeddings: 131072
rms_norm_eps: 1.0e-6
hidden_act: silu
attention_bias: false
qk_norm: true
rope_theta: 1000000.0
tie_word_embeddings: false
gradient_checkpointing: true
