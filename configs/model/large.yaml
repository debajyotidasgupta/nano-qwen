# Qwen3-235B-A22B MoE model
hidden_size: 4096
num_hidden_layers: 94
num_attention_heads: 64
num_key_value_heads: 4
head_dim: 128
intermediate_size: 12288
vocab_size: 151936
max_position_embeddings: 131072
rms_norm_eps: 1.0e-6
hidden_act: silu
attention_bias: false
qk_norm: true
rope_theta: 1000000.0
tie_word_embeddings: false

# MoE
num_experts: 128
num_experts_per_tok: 8
moe_intermediate_size: 1536
decoder_sparse_step: 1
router_aux_loss_coef: 0.001
norm_topk_prob: true

gradient_checkpointing: true
